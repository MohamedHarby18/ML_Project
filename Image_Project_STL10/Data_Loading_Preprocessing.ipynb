{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ebb998",
   "metadata": {},
   "source": [
    "# STL-10 Subset Classification: Optimized Preprocessing Pipeline\n",
    "\n",
    "## Dataset Overview\n",
    "- **Classes used:** airplane, bird, car, cat, deer (5 classes total)\n",
    "- **Source:** STL-10 dataset from Kaggle in binary format\n",
    "- **Image size:** Originally 96x96, resized to 64x64 for faster processing\n",
    "- **Train/Test split:** 80/20 after combining and shuffling all data\n",
    "\n",
    "## Preprocessing Pipeline\n",
    "1. Image loading and class filtering\n",
    "2. Grayscale conversion with CLAHE contrast enhancement\n",
    "3. Noise reduction with Gaussian blur\n",
    "4. Normalization and standardization\n",
    "5. Dimensionality reduction using Incremental PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abfbe2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Cell 1: Import Essential Libraries ================== #\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3a0ed",
   "metadata": {},
   "source": [
    "# 1️⃣ Load STL-10 Binary Data\n",
    "We use the STL-10 dataset downloaded from Kaggle in binary format. We filter the subset of 5 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9eae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading STL-10 dataset...\n",
      "Combining and filtering data...\n",
      "Shuffling and splitting...\n",
      "Train shape: (5200, 3, 96, 96), Test shape: (1300, 3, 96, 96)\n",
      "Train labels: 5200, Test labels: 1300\n"
     ]
    }
   ],
   "source": [
    "# ================== Cell 2: Load STL-10 subset (5 classes) ==================\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# --- Target Classes ---\n",
    "TARGET_CLASSES = [0, 1, 2, 3, 4]\n",
    "class_names = ['airplane', 'bird', 'car', 'cat', 'deer']\n",
    "\n",
    "# --- Simple Transform for Resizing ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),   # Resize to 64x64 for faster processing\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load STL-10 dataset\n",
    "print(\"Loading STL-10 dataset...\")\n",
    "train_set = datasets.STL10(root='../../Datasets', split='train', download=True, transform=transform)\n",
    "test_set = datasets.STL10(root='../../Datasets', split='test', download=True, transform=transform)\n",
    "\n",
    "# Combine and filter\n",
    "print(\"Combining and filtering data...\")\n",
    "combined_data = np.concatenate([train_set.data, test_set.data], axis=0)\n",
    "combined_labels = np.concatenate([train_set.labels, test_set.labels], axis=0)\n",
    "\n",
    "# Filter target classes\n",
    "indices = [i for i, label in enumerate(combined_labels) if label in TARGET_CLASSES]\n",
    "X = combined_data[indices]\n",
    "y = combined_labels[indices]\n",
    "\n",
    "# Re-map labels to 0..4\n",
    "label_map = {old: new for new, old in enumerate(TARGET_CLASSES)}\n",
    "y = np.array([label_map[label] for label in y])\n",
    "\n",
    "# Shuffle and split\n",
    "print(\"Shuffling and splitting...\")\n",
    "idx = np.arange(len(y))\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "split = int(0.8 * len(y))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"Train labels: {len(y_train)}, Test labels: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e453e08",
   "metadata": {},
   "source": [
    "# 2️⃣ Preprocessing\n",
    "- Resize images to 64x64  \n",
    "- Histogram Equalization per channel  \n",
    "- Normalize to [0,1]  \n",
    "- Flatten for feature matrix  \n",
    "- StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7782fe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying optimized preprocessing...\n",
      "After preprocessing - Train shape: (5200, 4096), Test shape: (1300, 4096)\n"
     ]
    }
   ],
   "source": [
    "# ================== Cell 3: Optimized Preprocessing ==================\n",
    "def optimized_preprocessing(images, target_size=64):\n",
    "    \"\"\"\n",
    "    Optimized preprocessing pipeline for STL-10 images\n",
    "    \n",
    "    Args:\n",
    "        images: Input images in (batch, channels, height, width) format\n",
    "        target_size: Target size for resizing\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed and flattened images\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    \n",
    "    for img in images:\n",
    "        # Convert from (channels, height, width) to (height, width, channels)\n",
    "        # for OpenCV processing\n",
    "        img_processed = np.transpose(img, (1, 2, 0))\n",
    "        \n",
    "        # 1. Resize to target size\n",
    "        if img_processed.shape[0] != target_size or img_processed.shape[1] != target_size:\n",
    "            img_processed = cv2.resize(img_processed, (target_size, target_size))\n",
    "        \n",
    "        # 2. Convert to grayscale (sufficient for traditional ML algorithms)\n",
    "        gray = cv2.cvtColor(img_processed, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # 3. Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "        # Better than standard histogram equalization for preserving details\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        gray = clahe.apply(gray)\n",
    "        \n",
    "        # 4. Apply mild Gaussian blur to reduce noise\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        \n",
    "        # 5. Normalize to [0, 1] range\n",
    "        gray = gray.astype(np.float32) / 255.0\n",
    "        \n",
    "        processed_images.append(gray.flatten())  # Flatten directly\n",
    "    \n",
    "    return np.array(processed_images)\n",
    "\n",
    "print(\"\\nApplying optimized preprocessing...\")\n",
    "Xtr_processed = optimized_preprocessing(X_train)\n",
    "Xte_processed = optimized_preprocessing(X_test)\n",
    "\n",
    "print(f\"After preprocessing - Train shape: {Xtr_processed.shape}, Test shape: {Xte_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666ad17",
   "metadata": {},
   "source": [
    "# 3️⃣ Feature Extraction (PCA)\n",
    "Reduce dimensions to 200 principal components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79379193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying feature standardization...\n",
      "Scaled features - Mean: 0.000000, Std: 1.000000\n",
      "Features per image: 4096\n"
     ]
    }
   ],
   "source": [
    "# ================== Cell 4: Feature Standardization ==================\n",
    "print(\"\\nApplying feature standardization...\")\n",
    "scaler = StandardScaler()\n",
    "Xtr_scaled = scaler.fit_transform(Xtr_processed)\n",
    "Xte_scaled = scaler.transform(Xte_processed)\n",
    "\n",
    "print(f\"Scaled features - Mean: {Xtr_scaled.mean():.6f}, Std: {Xtr_scaled.std():.6f}\")\n",
    "print(f\"Features per image: {Xtr_scaled.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37786cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Incremental PCA...\n",
      "PCA Parameters: n_components=200, batch_size=500\n",
      "  Fitted 2500/5200 samples\n",
      "  Fitted 5000/5200 samples\n",
      "  Fitted 5200/5200 samples\n",
      "\n",
      "Transforming data...\n",
      "\n",
      "PCA Transformation Complete!\n",
      "  Original shape: (5200, 4096)\n",
      "  Reduced shape: (5200, 200)\n",
      "  Explained variance: 0.8800\n",
      "  Number of components: 200\n",
      "\n",
      "Cleaning up memory...\n",
      "\n",
      "Preprocessing pipeline completed successfully!\n",
      "Final training data shape: (5200, 200)\n",
      "Final test data shape: (1300, 200)\n"
     ]
    }
   ],
   "source": [
    "# ================== Cell 5: Efficient Dimensionality Reduction ==================\n",
    "print(\"\\nApplying Incremental PCA...\")\n",
    "\n",
    "# Determine optimal number of components (keep 95% variance)\n",
    "n_components = min(200, Xtr_scaled.shape[1] - 1)\n",
    "batch_size = min(500, Xtr_scaled.shape[0])\n",
    "\n",
    "print(f\"PCA Parameters: n_components={n_components}, batch_size={batch_size}\")\n",
    "\n",
    "# Initialize and fit PCA\n",
    "pca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "\n",
    "# Fit PCA incrementally\n",
    "n_batches = int(np.ceil(Xtr_scaled.shape[0] / batch_size))\n",
    "for i in range(n_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, Xtr_scaled.shape[0])\n",
    "    batch = Xtr_scaled[start_idx:end_idx]\n",
    "    pca.partial_fit(batch)\n",
    "    \n",
    "    if (i + 1) % 5 == 0 or (i + 1) == n_batches:\n",
    "        print(f\"  Fitted {end_idx}/{Xtr_scaled.shape[0]} samples\")\n",
    "\n",
    "# Transform data\n",
    "print(\"\\nTransforming data...\")\n",
    "Xtr_pca = pca.transform(Xtr_scaled)\n",
    "Xte_pca = pca.transform(Xte_scaled)\n",
    "\n",
    "print(f\"\\nPCA Transformation Complete!\")\n",
    "print(f\"  Original shape: {Xtr_scaled.shape}\")\n",
    "print(f\"  Reduced shape: {Xtr_pca.shape}\")\n",
    "print(f\"  Explained variance: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "print(f\"  Number of components: {pca.n_components_}\")\n",
    "\n",
    "# Memory cleanup\n",
    "print(\"\\nCleaning up memory...\")\n",
    "del Xtr_processed, Xte_processed, Xtr_scaled\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nPreprocessing pipeline completed successfully!\")\n",
    "print(f\"Final training data shape: {Xtr_pca.shape}\")\n",
    "print(f\"Final test data shape: {Xte_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc90b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving processed data for use in modeling notebook...\n",
      "Data saved successfully!\n",
      "Files saved in '../data/processed/':\n",
      "  - X_train_pca.npy: PCA-transformed training features\n",
      "  - X_test_pca.npy: PCA-transformed test features\n",
      "  - y_train.npy: Training labels\n",
      "  - y_test.npy: Test labels\n",
      "  - class_names.npy: Class names for reference\n",
      "\n",
      "Models saved in '../models/':\n",
      "  - scaler.pkl: StandardScaler for consistency\n",
      "  - pca_model.pkl: PCA model for transforming new data\n"
     ]
    }
   ],
   "source": [
    "# ================== Cell 6: Save Processed Data for Next Notebook ==================\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"\\nSaving processed data for use in modeling notebook...\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save processed data\n",
    "np.save('../data/processed/X_train_pca.npy', Xtr_pca)\n",
    "np.save('../data/processed/X_test_pca.npy', Xte_pca)\n",
    "np.save('../data/processed/y_train.npy', y_train)\n",
    "np.save('../data/processed/y_test.npy', y_test)\n",
    "\n",
    "# Save the scaler and PCA models for consistency\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "joblib.dump(pca, '../models/pca_model.pkl')\n",
    "\n",
    "# Save class names for reference\n",
    "np.save('../data/processed/class_names.npy', np.array(class_names))\n",
    "\n",
    "print(\"Data saved successfully!\")\n",
    "print(f\"Files saved in '../data/processed/':\")\n",
    "print(\"  - X_train_pca.npy: PCA-transformed training features\")\n",
    "print(\"  - X_test_pca.npy: PCA-transformed test features\")\n",
    "print(\"  - y_train.npy: Training labels\")\n",
    "print(\"  - y_test.npy: Test labels\")\n",
    "print(\"  - class_names.npy: Class names for reference\")\n",
    "print(\"\\nModels saved in '../models/':\")\n",
    "print(\"  - scaler.pkl: StandardScaler for consistency\")\n",
    "print(\"  - pca_model.pkl: PCA model for transforming new data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
